# Local LLM Setup Guide

Run AI email analysis completely offline using your own GPU! No API costs, full privacy, unlimited usage.

## Your Hardware: Perfect for Local AI

**Your specs:**
- üéÆ GPU: 16GB VRAM
- üíæ RAM: 96GB
- ‚úÖ **Excellent for running 70B models!**

## Quick Start with Ollama

### 1. Install Ollama

**Linux:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

**macOS:**
```bash
brew install ollama
```

**Windows:**
Download from https://ollama.com/download

### 2. Pull a Model

**Recommended for your hardware (16GB VRAM):**

```bash
# Best quality - Llama 3.1 70B (quantized Q4)
ollama pull llama3.1:70b-instruct-q4_K_M

# Excellent alternative - Qwen 2.5 32B
ollama pull qwen2.5:32b-instruct-q4_K_M

# Fast and capable - Llama 3.2 11B
ollama pull llama3.2:11b-instruct-q4_K_M

# Lighter option - Mistral 7B
ollama pull mistral:7b-instruct-q4_K_M
```

### 3. Start Ollama Server

```bash
ollama serve
```

(Usually starts automatically after install)

### 4. Configure MailCow AI Filter

Edit `config/config.yml`:

```yaml
ai:
  provider: "ollama"
  model: "llama3.1:70b-instruct-q4_K_M"
  base_url: "http://localhost:11434"
```

### 5. Run

```bash
docker-compose up
```

## Model Recommendations

### For 16GB VRAM (Your Setup)

| Model | VRAM | Quality | Speed | Best For |
|-------|------|---------|-------|----------|
| **Llama 3.1 70B Q4** | ~14GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Medium | Best overall quality |
| **Qwen 2.5 32B Q4** | ~9GB | ‚≠ê‚≠ê‚≠ê‚≠ê | Fast | Great multilingual |
| **Llama 3.2 11B Q4** | ~7GB | ‚≠ê‚≠ê‚≠ê | Very Fast | Quick analysis |
| **Mistral 7B Q4** | ~4GB | ‚≠ê‚≠ê‚≠ê | Ultra Fast | Testing/lightweight |

### Recommended Choice: Llama 3.1 70B

```bash
ollama pull llama3.1:70b-instruct-q4_K_M
```

**Why:**
- Excellent reasoning and pattern recognition
- Fits perfectly in 16GB VRAM
- Comparable to Claude Sonnet quality
- Great at following JSON output format

## Model Details

### Llama 3.1 70B (Recommended)

**Full name:** `llama3.1:70b-instruct-q4_K_M`

**Stats:**
- Parameters: 70 billion
- VRAM: ~14GB (Q4 quantized)
- Speed: 2-5 tokens/sec on RTX 3090/4090
- Quality: Excellent for email categorization

**Pros:**
- ‚úÖ Top-tier reasoning
- ‚úÖ Reliable JSON output
- ‚úÖ Great pattern detection
- ‚úÖ Fits your hardware perfectly

**Cons:**
- ‚è±Ô∏è Slower inference (~1-3 minutes per analysis)

### Qwen 2.5 32B

**Full name:** `qwen2.5:32b-instruct-q4_K_M`

**Stats:**
- Parameters: 32 billion
- VRAM: ~9GB
- Speed: 5-10 tokens/sec
- Quality: Very good

**Pros:**
- ‚úÖ Faster than 70B
- ‚úÖ Excellent multilingual support
- ‚úÖ Good at structured output
- ‚úÖ Leaves VRAM headroom

**Cons:**
- ‚ö†Ô∏è Slightly less capable than 70B models

### Llama 3.2 11B

**Full name:** `llama3.2:11b-instruct-q4_K_M`

**Stats:**
- Parameters: 11 billion
- VRAM: ~7GB
- Speed: 10-20 tokens/sec
- Quality: Good

**Pros:**
- ‚úÖ Very fast
- ‚úÖ Low VRAM usage
- ‚úÖ Still quite capable

**Cons:**
- ‚ö†Ô∏è Less sophisticated reasoning
- ‚ö†Ô∏è May miss subtle patterns

### Mistral 7B

**Full name:** `mistral:7b-instruct-q4_K_M`

**Stats:**
- Parameters: 7 billion
- VRAM: ~4GB
- Speed: 15-30 tokens/sec
- Quality: Good for its size

**Pros:**
- ‚úÖ Ultra fast
- ‚úÖ Minimal VRAM
- ‚úÖ Good for testing

**Cons:**
- ‚ö†Ô∏è Basic pattern recognition
- ‚ö†Ô∏è May struggle with complex categorization

## Configuration Examples

### Best Quality (Recommended)

```yaml
ai:
  provider: "ollama"
  model: "llama3.1:70b-instruct-q4_K_M"
  base_url: "http://localhost:11434"
  max_emails_to_analyze: 100
```

### Balanced (Fast & Good)

```yaml
ai:
  provider: "ollama"
  model: "qwen2.5:32b-instruct-q4_K_M"
  base_url: "http://localhost:11434"
  max_emails_to_analyze: 100
```

### Speed Priority

```yaml
ai:
  provider: "ollama"
  model: "llama3.2:11b-instruct-q4_K_M"
  base_url: "http://localhost:11434"
  max_emails_to_analyze: 100
```

## Performance Expectations

### Analysis Time (100 emails)

| Model | Time | Quality |
|-------|------|---------|
| Llama 3.1 70B | 1-3 min | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Qwen 2.5 32B | 30-90 sec | ‚≠ê‚≠ê‚≠ê‚≠ê |
| Llama 3.2 11B | 15-45 sec | ‚≠ê‚≠ê‚≠ê |
| Mistral 7B | 10-30 sec | ‚≠ê‚≠ê‚≠ê |

**Note:** Times vary based on GPU (RTX 3090/4090/etc.)

## Advantages of Local Models

### 1. Zero API Costs
- ‚úÖ No per-run charges
- ‚úÖ Unlimited usage
- ‚úÖ One-time setup

### 2. Complete Privacy
- ‚úÖ Emails never leave your server
- ‚úÖ No data sent to third parties
- ‚úÖ Full control

### 3. No Rate Limits
- ‚úÖ Run as many times as you want
- ‚úÖ No API quotas
- ‚úÖ No waiting

### 4. Offline Operation
- ‚úÖ Works without internet
- ‚úÖ No dependency on external services
- ‚úÖ Always available

## Disadvantages vs Claude API

### Quality
- Claude Opus/Sonnet may be slightly better
- Local 70B models are very close in quality
- For email categorization, 70B is sufficient

### Speed
- Claude API: 10-20 seconds
- Local 70B: 1-3 minutes
- Still acceptable for batch analysis

### Convenience
- Ollama requires ~40GB disk space for 70B model
- Need to manage model updates
- Slightly more complex setup

## Docker Integration

### Option 1: Ollama on Host

Run Ollama on your host machine:

```bash
ollama serve
```

Configure Docker to access host:

```yaml
# docker-compose.yml
services:
  mailcow-ai-filter:
    network_mode: "host"  # Access host's Ollama
    # Or use extra_hosts:
    extra_hosts:
      - "host.docker.internal:host-gateway"
```

Update config:
```yaml
ai:
  base_url: "http://host.docker.internal:11434"
```

### Option 2: Ollama in Docker (with GPU)

**docker-compose.yml:**

```yaml
version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  mailcow-ai-filter:
    build: .
    depends_on:
      - ollama
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434

volumes:
  ollama_data:
```

## Troubleshooting

### Ollama Not Found

**Error:** `Failed to connect to Ollama at http://localhost:11434`

**Solution:**
```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# Start Ollama
ollama serve

# Or restart service
systemctl restart ollama
```

### Model Not Found

**Error:** `Model llama3.1:70b not found`

**Solution:**
```bash
# List available models
ollama list

# Pull the model
ollama pull llama3.1:70b-instruct-q4_K_M
```

### Out of Memory

**Error:** `CUDA out of memory`

**Solution:**
1. Use smaller model:
   ```bash
   ollama pull qwen2.5:32b-instruct-q4_K_M
   ```

2. Or lower quantization:
   ```bash
   ollama pull llama3.1:70b-instruct-q3_K_M  # Q3 instead of Q4
   ```

### Slow Performance

**Issue:** Takes 10+ minutes

**Solutions:**
- Check GPU is being used: `nvidia-smi`
- Try smaller model for speed
- Ensure no other GPU processes running
- Check CUDA/GPU drivers installed

### Poor Quality Output

**Issue:** Categories don't make sense

**Solutions:**
- Use larger model (70B instead of 7B)
- Increase sample size
- Try different model (Qwen vs Llama)
- Check prompt in logs

## Comparing to Claude

### Quality Comparison

| Task | Claude Sonnet | Llama 3.1 70B | Qwen 2.5 32B |
|------|--------------|---------------|--------------|
| Pattern Detection | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| JSON Format | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| Email Understanding | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| Consistency | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |

**Verdict:** Local 70B models are 85-90% as good as Claude Sonnet for this task.

## Best Practices

### 1. Start with Testing

```bash
# Pull smaller model first
ollama pull mistral:7b-instruct-q4_K_M

# Test with small sample
# Set max_emails_to_analyze: 20

# If works well, upgrade to 70B
```

### 2. Monitor Resources

```bash
# Watch GPU usage
watch -n 1 nvidia-smi

# Check memory
htop
```

### 3. Model Selection Strategy

- **First time:** Start with Mistral 7B (fast testing)
- **Quality matters:** Upgrade to Llama 3.1 70B
- **Speed matters:** Use Qwen 2.5 32B
- **Best of both:** Qwen 2.5 32B

### 4. Optimize for Your Use Case

**One-time setup:**
- Use 70B for best quality
- Take your time (1-3 minutes is fine)

**Frequent re-runs:**
- Use 32B for faster iterations
- Still good quality

## Advanced: Custom Models

Want to try other models?

```bash
# List all available models
ollama list

# Search Ollama library
# Visit: https://ollama.com/library

# Pull any model
ollama pull model-name:tag
```

Popular alternatives:
- `deepseek-coder-v2:16b` - Code understanding
- `phi3:14b` - Microsoft's efficient model
- `mixtral:8x7b` - Mixture of experts

## Cost Comparison

### One-Time Analysis (100 emails)

| Option | Cost | Time |
|--------|------|------|
| **Claude Sonnet** | $0.12 | 15 sec |
| **Local Llama 70B** | $0.00 | 2 min |

### Monthly (4 runs)

| Option | Cost | Time |
|--------|------|------|
| **Claude Sonnet** | $0.48 | 1 min |
| **Local Llama 70B** | $0.00 | 8 min |

**One-time setup costs:**
- Ollama: Free
- Model download: Free (~40GB disk)
- GPU: Already owned

**Recommendation for your setup:**
Use **Local Llama 3.1 70B** for zero ongoing costs and full privacy!

## Summary

With your 16GB VRAM GPU, you're perfectly positioned for local AI:

‚úÖ **Best Choice:** Llama 3.1 70B Q4
‚úÖ **Zero API costs**
‚úÖ **Complete privacy**
‚úÖ **Unlimited usage**
‚úÖ **Quality comparable to Claude**

Get started:
```bash
# Install
curl -fsSL https://ollama.com/install.sh | sh

# Pull model
ollama pull llama3.1:70b-instruct-q4_K_M

# Configure
# Set provider: "ollama" in config.yml

# Run
docker-compose up
```

Happy local AI email sorting! üöÄ
